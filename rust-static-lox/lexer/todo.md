# Complete Rust Lexer Implementation TODO

A comprehensive checklist for building a production-ready Rust lexer from scratch.

---

## Phase 1: Core Infrastructure üèóÔ∏è

### 1.1 Basic Lexer Structure
- [x] Create `Lexer` struct with:
  - [x] `source: SourceFile` - Source file with content
  - [x] `current: usize` - Current byte position
  - [x] `start: usize` - Start byte position of current token
  - [x] `line: usize` - Current line (1-indexed)
  - [x] `column: usize` - Current column (1-indexed)
- [x] Implement `Lexer::new(source: SourceFile) -> Self`
- [x] Implement `Lexer::scan_tokens()` (main entry point)
- [x] Add helper methods:
  - [x] `peek() -> Option<char>` - Look at current char without consuming
  - [x] `peek_next(offset: usize) -> Option<char>` - Look ahead n characters
  - [x] `advance() -> char` - Advance and return current char
  - [x] `get_current_offset() -> usize` - Get current byte position
  - [x] `get_current_lexeme() -> &str` - Get current token text

### 1.2 Span/Location Tracking
- [x] Implement span calculation (start/end positions)
- [x] Add line and column tracking (optional, for better error messages)
- [x] Create `Span::new(start: usize, end: usize)` constructor
- [x] Implement `Span::merge()` for combining spans

---

## Phase 2: Whitespace & Comments üìù

### 2.1 Whitespace
- [x] Implement `lex_whitespace() -> Token`
- [x] Handle all Unicode whitespace characters:
  - [x] Space (` `)
  - [x] Tab (`\t`)
  - [x] Newline (`\n`)
  - [x] Carriage return (`\r`)
  - [x] Other Unicode whitespace (use `char::is_whitespace()`)
- [x] Test with mixed whitespace

### 2.2 Line Comments
- [x] Implement `lex_line_comment() -> Token`
- [x] Detect `//` at start
- [x] Check for doc comments:
  - [x] `///` ‚Üí `DocStyle::Outer`
  - [x] `//!` ‚Üí `DocStyle::Inner`
  - [x] `//` ‚Üí `None` (regular comment)
- [x] Consume until end of line (not including `\n`)
- [x] Handle EOF in middle of line comment

### 2.3 Block Comments
- [x] Implement `lex_block_comment() -> Token`
- [x] Detect `/*` at start
- [x] Check for doc comments:
  - [x] `/**` (not `/**/`) ‚Üí `DocStyle::Outer`
  - [x] `/*!` ‚Üí `DocStyle::Inner`
  - [x] `/*` ‚Üí `None` (regular comment)
- [x] **Handle nesting**: `/* /* nested */ */`
  - [x] Track nesting depth counter
  - [x] Increment on `/*`, decrement on `*/`
- [x] Set `terminated: false` if EOF before closing `*/`
- [x] Test deeply nested comments

### 2.4 Shebang
- [x] Implement `lex_shebang() -> Option<Token>`
- [x] **Only valid as first token** (position 0)
- [x] Must start with `#!`
- [x] Must be followed by `[` or `/` (not `![` for inner attribute)
- [x] Consume until end of line
- [x] Return `None` if not at position 0 or invalid

---

## Phase 3: Identifiers & Keywords üî§

### 3.1 Regular Identifiers
- [x] Implement `lex_ident() -> Token` (via `lex_keywords()`)
- [x] Validate first character:
  - [x] `a-z`, `A-Z`, `_`
  - [ ] Unicode XID_Start characters (ASCII only currently)
- [x] Validate continuation characters:
  - [x] `a-z`, `A-Z`, `0-9`, `_`
  - [ ] Unicode XID_Continue characters (ASCII only currently)
- [ ] Use `char::is_xid_start()` and `char::is_xid_continue()` (currently ASCII only)
- [x] Mark as `InvalidIdent` if contains invalid characters (like starting with digits)

### 3.2 Raw Identifiers
- [x] Implement `lex_raw_ident() -> Token` (via `lex_keywords()`)
- [x] Detect `r#` prefix
- [x] Parse identifier after `r#`
- [x] Validate that it's a valid identifier
- [ ] Special cases:
  - [ ] `r#_` is invalid (underscore alone cannot be raw ident)
  - [ ] `r#crate` has special meaning

### 3.3 Keywords (Parser-level, but document here)
- [x] Document that lexer emits keyword tokens (not `Ident`)
- [x] Lexer checks against keyword list:
  - [x] Strict keywords: `as`, `break`, `const`, `continue`, `crate`, `else`, `enum`, `extern`, `false`, `fn`, `for`, `if`, `impl`, `in`, `let`, `loop`, `match`, `mod`, `move`, `mut`, `pub`, `ref`, `return`, `self`, `Self`, `static`, `struct`, `super`, `trait`, `true`, `type`, `unsafe`, `use`, `where`, `while`
  - [x] Reserved keywords: `abstract`, `become`, `box`, `do`, `final`, `macro`, `override`, `priv`, `typeof`, `unsized`, `virtual`, `yield`
  - [x] Weak keywords (contextual): `async`, `await`, `dyn`, `try`, `union`

---

## Phase 4: Lifetimes üïê

### 4.1 Regular Lifetimes
- [ ] Implement `lex_lifetime() -> Token`
- [ ] Detect `'` followed by identifier
- [ ] Parse identifier part (same rules as regular idents)
- [ ] Check if starts with number:
  - [ ] `'1abc` ‚Üí set `starts_with_number: true` (invalid)
- [ ] Special lifetimes:
  - [ ] `'_` (anonymous lifetime)
  - [ ] `'static` (special, but treated as regular ident)

### 4.2 Raw Lifetimes
- [ ] Implement `lex_raw_lifetime() -> Token`
- [ ] Detect `'r#` prefix
- [ ] Parse identifier after `'r#`
- [ ] Allow keywords as lifetime names

### 4.3 Lifetime vs Char Literal Disambiguation
- [ ] If `'` followed by identifier start ‚Üí lifetime
- [ ] If `'` followed by anything else ‚Üí try char literal
- [ ] Handle edge cases like `'1` (invalid lifetime)

---

## Phase 5: Numeric Literals üî¢

### 5.1 Integer Literals
- [x] Implement `lex_number() -> Token`
- [x] Detect base prefix:
  - [x] `0b` ‚Üí Binary
  - [x] `0o` ‚Üí Octal
  - [x] `0x` ‚Üí Hexadecimal
  - [x] None ‚Üí Decimal
- [x] Parse digits according to base:
  - [x] Binary: `0-1`
  - [x] Octal: `0-7`
  - [x] Decimal: `0-9`
  - [x] Hex: `0-9`, `a-f`, `A-F`
- [x] Handle digit separators: `1_000_000`
- [x] Set `empty_int: true` if no digits after prefix (e.g., `0x`)
- [ ] Parse optional suffix: `u8`, `i32`, `u64`, `i128`, `usize`, `isize` (suffix_start tracked but not parsed)

### 5.2 Float Literals
- [x] Detect float when number contains:
  - [x] Decimal point: `3.14`
  - [x] Exponent: `1e10`, `2E-5`
- [x] Parse decimal point and fractional part
- [x] Parse exponent:
  - [x] `e` or `E` followed by optional `+`/`-`
  - [x] Then digits
  - [x] Set `empty_exponent: true` if no digits after `e`
- [ ] Parse optional suffix: `f32`, `f64` (suffix_start tracked but not parsed)
- [x] **Special case**: `1f32` is int with suffix, not float (handled correctly)
- [ ] Hexadecimal floats (rare): `0x1.8p3` (not implemented)

### 5.3 Edge Cases
- [x] `123` ‚Üí Int (decimal)
- [x] `0x` ‚Üí Int with `empty_int: true`
- [x] `1.` ‚Üí Float (with empty fractional part)
- [x] `1e` ‚Üí Float with `empty_exponent: true`
- [ ] `1.foo()` ‚Üí Int `1`, then `.`, then method call (not a float!) (needs parser coordination)
- [ ] `1.2.3` ‚Üí Float `1.2`, then `.`, then `3` (needs parser coordination)

---

## Phase 6: Character & Byte Literals üìù

### 6.1 Character Literals
- [x] Implement `lex_char() -> Token`
- [x] Detect opening `'`
- [x] Parse content:
  - [x] Regular character: `'a'`
  - [x] Escape sequences (see 6.3)
  - [x] Unicode escapes: `'\u{1F980}'`
- [x] Detect closing `'`
- [x] Set `terminated: false` if EOF or newline before closing
- [x] Validate:
  - [x] Not empty: `''` is invalid (handled by terminated flag)
  - [ ] Single character (after escaping) (validation in parser)

### 6.2 Byte Literals
- [x] Implement `lex_byte() -> Token` (via `lex_bchar()`)
- [x] Detect `b'` prefix
- [x] Parse ASCII-only content
- [x] Same escape rules as char, but restricted to ASCII
- [x] Validate byte is in range 0-127 (after escaping)

### 6.3 Escape Sequences
- [x] Implement escape sequence parsing (inline in lexers)
- [x] Simple escapes:
  - [x] `\n` ‚Üí newline
  - [x] `\r` ‚Üí carriage return
  - [x] `\t` ‚Üí tab
  - [x] `\\` ‚Üí backslash
  - [x] `\'` ‚Üí single quote
  - [x] `\"` ‚Üí double quote
  - [x] `\0` ‚Üí null
- [x] Byte escapes: `\xHH` (2 hex digits)
- [x] Unicode escapes: `\u{HHHHHH}` (1-6 hex digits)
- [x] Validate Unicode scalar values (not surrogate pairs)

---

## Phase 7: String Literals üìú

### 7.1 Regular Strings
- [x] Implement `lex_string() -> Token` (via `lex_str()`)
- [x] Detect opening `"`
- [x] Parse content with escape sequences
- [x] Allow multi-line strings
- [x] Detect closing `"`
- [x] Set `terminated: false` if EOF before closing

### 7.2 Raw Strings
- [x] Implement `lex_raw_string() -> Token` (via `lex_raw_str()`)
- [x] Detect `r` followed by zero or more `#`, then `"`
- [x] Count opening `#` characters ‚Üí `n_hashes`
- [x] Validate no other characters between `r` and `#*"` (emits diagnostic)
- [x] Parse content (no escapes processed)
- [x] Find closing: `"` followed by same number of `#`
- [x] Set `terminated: false` if:
  - [x] EOF reached
  - [x] Different number of closing `#`
- [x] Set `n_hashes` capped at 255 if > 255 (emits diagnostic)

### 7.3 Byte Strings
- [x] Implement `lex_byte_string() -> Token` (via `lex_bstr()`)
- [x] Detect `b"` prefix
- [x] Same as regular strings but ASCII-only
- [x] Validate all bytes are 0-127 (after escaping)

### 7.4 Raw Byte Strings
- [x] Implement `lex_raw_byte_string() -> Token` (via `lex_bstr()`)
- [x] Detect `br` prefix
- [x] Combine raw string and byte string rules
- [x] No escapes, ASCII-only content

### 7.5 C Strings (Rust 1.77+)
- [x] Implement `lex_c_string() -> Token` (via `lex_cstr()`)
- [x] Detect `c"` prefix
- [x] Same as regular strings
- [x] Automatically null-terminated (semantic, not lexer concern)
- [ ] Cannot contain interior `\0` (except as last char) (validation in parser)

### 7.6 Raw C Strings
- [x] Implement `lex_raw_c_string() -> Token` (via `lex_craw_str()`)
- [x] Detect `cr` prefix
- [x] Combine raw string and C string rules

---

## Phase 8: Operators & Punctuation ‚öôÔ∏è

### 8.1 Single-Character Tokens
Implement individual lexing functions for each:
- [x] `;` ‚Üí `Semi`
- [x] `,` ‚Üí `Comma`
- [x] `.` ‚Üí `Dot` (check not start of `..` or number)
- [x] `(` ‚Üí `OpenParen`
- [x] `)` ‚Üí `CloseParen`
- [x] `{` ‚Üí `OpenBrace`
- [x] `}` ‚Üí `CloseBrace`
- [x] `[` ‚Üí `OpenBracket`
- [x] `]` ‚Üí `CloseBracket`
- [x] `@` ‚Üí `At`
- [x] `#` ‚Üí `Pound`
- [x] `~` ‚Üí `Tilde`
- [x] `?` ‚Üí `Question`
- [x] `:` ‚Üí `Colon`
- [x] `$` ‚Üí `Dollar`
- [x] `=` ‚Üí `Eq` (also handles `==`, `=>`)
- [x] `!` ‚Üí `Bang` (also handles `!=`)
- [x] `<` ‚Üí `Lt` (also handles `<=`, `<<`, `<<=`)
- [x] `>` ‚Üí `Gt` (also handles `>=`, `>>`, `>>=`)
- [x] `-` ‚Üí `Minus` (also handles `-=`, `->`)
- [x] `&` ‚Üí `And` (also handles `&&`, `&=`)
- [x] `|` ‚Üí `Or` (also handles `||`, `|=`)
- [x] `+` ‚Üí `Plus` (also handles `+=`)
- [x] `*` ‚Üí `Star` (also handles `*=`)
- [x] `/` ‚Üí `Slash` (also handles `/=`, comments)
- [x] `^` ‚Üí `Caret` (also handles `^=`)
- [x] `%` ‚Üí `Percent` (also handles `%=`)
- [x] `::` ‚Üí `ColonColon`
- [x] `..` ‚Üí `DotDot`
- [x] `..=` ‚Üí `DotDotEq`
- [ ] `...` ‚Üí `DotDotDot` (deprecated, not implemented)

### 8.2 Disambiguation
- [x] `/` ‚Üí Check if followed by `/` (line comment) or `*` (block comment)
- [ ] `.` ‚Üí Check if followed by digit (float literal) (handled in number lexer)
- [ ] `'` ‚Üí Check if lifetime or char literal (currently only char literal)
- [x] `#` ‚Üí Check if `#!` at position 0 (shebang)
- [x] Letters ‚Üí Check if prefix for literal (`b`, `r`, `br`, `c`, `cr`)

---

## Phase 9: Unknown Prefixes & Reserved Syntax üö´

### 9.1 Unknown Literal Prefixes
- [ ] Implement `detect_unknown_prefix() -> Option<Token>`
- [ ] If identifier followed immediately by `"`, `'`, or `#"`:
  - [ ] Check if it's a known prefix (`b`, `r`, `br`, `c`, `cr`)
  - [ ] If not known ‚Üí `UnknownPrefix`
  - [ ] Token contains only the prefix part

### 9.2 Unknown Lifetime Prefixes
- [ ] Implement `detect_unknown_lifetime_prefix() -> Option<Token>`
- [ ] If `'` + identifier + `#`:
  - [ ] Not `'r#` ‚Üí `UnknownPrefixLifetime`

### 9.3 Reserved Prefixes (Rust 2024+)
- [ ] Implement `detect_reserved_prefix() -> Option<Token>`
- [ ] Check for single-letter + `#` patterns: `k#`, `f#`
- [ ] Reserved for future literal types
- [ ] Return `ReservedPrefix` token

---

## Phase 10: Main Lexer Loop üîÑ

### 10.1 Token Dispatch
- [x] Implement main `scan_tokens()` logic:
  ```rust
  while !is_eof() {
      let c = advance();
      let token = lex_tokens(c, engine);
      if let Some(token) = token {
          emit(token);
      }
  }
  emit(Eof);
  ```

### 10.2 Character-based Dispatch
- [x] Whitespace chars ‚Üí `lex_whitespace()`
- [x] `/` ‚Üí Check for comments or `Slash`
- [x] `#` ‚Üí Check for shebang or `Pound`
- [x] `'` ‚Üí Check for char literal (lifetime not yet implemented)
- [x] `"` ‚Üí `lex_string()`
- [x] `a-z`, `A-Z`, `_` ‚Üí `lex_keywords()` or check prefix
- [x] `0-9` ‚Üí `lex_number()`
- [x] Operators ‚Üí Single-char tokens
- [ ] Everything else ‚Üí `Unknown` (currently returns None with diagnostic)

### 10.3 Prefix Detection
- [x] After lexing potential identifier, check next char:
  - [x] `"` ‚Üí Could be string prefix (handled in `lex_string()`)
  - [x] `'` ‚Üí Could be byte literal prefix (handled in `lex_string()`)
  - [x] `#` ‚Üí Could be raw string prefix (handled in `lex_string()`)
- [x] Uses lookahead via `peek()` and `peek_next()`

---

## Phase 11: Error Handling & Validation ‚ö†Ô∏è

### 11.1 Malformed Literals
- [x] Unterminated strings/chars: set `terminated: false`
- [x] Invalid escape sequences: emits diagnostics
- [x] Empty numbers after prefix: set `empty_int: true`
- [x] Invalid raw string delimiters: emits diagnostics, caps n_hashes

### 11.2 Invalid Characters
- [ ] Return `Unknown` token for unrecognized characters (currently returns None)
- [x] Unicode characters not valid in identifiers ‚Üí `InvalidIdent` (for starting with digits)
- [x] Non-ASCII in byte literals ‚Üí validation error (emits diagnostic)

### 11.3 Contextual Validation
- [x] Shebang only at position 0
- [ ] Raw identifiers cannot be `_` alone (not validated)
- [ ] Lifetimes cannot start with numbers (not implemented)

---

## Phase 12: Testing üß™

### 12.1 Unit Tests Per Feature
- [ ] Whitespace: various Unicode whitespace
- [ ] Comments: nested, unterminated, doc comments
- [ ] Identifiers: ASCII, Unicode, keywords, raw
- [ ] Lifetimes: regular, raw, invalid
- [ ] Numbers: all bases, floats, empty, suffixes
- [ ] Chars/bytes: escapes, unicode, unterminated
- [ ] Strings: all variants, raw with different `#` counts
- [ ] Operators: all single-char tokens
- [ ] Unknown/invalid tokens

### 12.2 Integration Tests
- [ ] Lex complete Rust files from standard library
- [ ] Lex code with intentional errors
- [ ] Lex edge cases: empty file, only whitespace, only comments
- [ ] Performance test: large files (1MB+)

### 12.3 Fuzzing
- [ ] Set up cargo-fuzz
- [ ] Fuzz with random bytes
- [ ] Fuzz with semi-valid Rust code
- [ ] Check for panics and infinite loops

---

## Phase 13: Optimizations üöÄ

### 13.1 Performance
- [ ] Profile hot paths with `cargo flamegraph`
- [ ] Optimize character lookahead (avoid repeated UTF-8 decoding)
- [ ] Use byte-based matching where possible (ASCII fast path)
- [ ] Consider SIMD for whitespace/identifier scanning

### 13.2 Memory
- [ ] Minimize allocations in hot path
- [ ] Reuse buffers where possible
- [ ] Consider arena allocation for tokens

### 13.3 Benchmarking
- [ ] Set up criterion benchmarks
- [ ] Benchmark against rustc_lexer
- [ ] Test on various file sizes and code styles

---

## Phase 14: API & Documentation üìö

### 14.1 Public API
- [x] Clean, ergonomic API for consumers:
  ```rust
  let mut lexer = Lexer::new(source);
  lexer.scan_tokens(&mut engine);
  let tokens = lexer.tokens;
  ```
- [ ] Iterator implementation (not implemented)
- [x] Error recovery mode (continue after errors via diagnostics)

### 14.2 Documentation
- [x] Document all public types and methods
- [x] Add examples to struct/function docs
- [ ] Create comprehensive README
- [ ] Document differences from rustc_lexer (if any)

### 14.3 Examples
- [ ] Simple token printer example
- [ ] Syntax highlighter example
- [ ] Token statistics analyzer

---

## Phase 15: Advanced Features (Optional) ‚ú®

### 15.1 Error Recovery
- [ ] Continue lexing after errors
- [ ] Produce sensible tokens for malformed input
- [ ] Provide helpful error messages

### 15.2 Source Maps
- [ ] Track original source locations
- [ ] Support for `#[macro_export]` and `include!()` expansions
- [ ] Map tokens back to original files

### 15.3 Proc Macro Support
- [ ] TokenStream compatible output
- [ ] Preserve token spacing information
- [ ] Support token tree construction

### 15.4 IDE Support
- [ ] Provide token semantic information
- [ ] Support incremental lexing
- [ ] Token classification for syntax highlighting

---

## Phase 16: Validation & Release üéâ

### 16.1 Compliance Testing
- [ ] Test against Rust specification
- [ ] Compare output with rustc_lexer on Rust repo
- [ ] Test edition-specific features (2015, 2018, 2021, 2024)

### 16.2 Code Quality
- [ ] Run clippy with strict lints
- [ ] Ensure no unsafe code (or justify it)
- [ ] 100% documentation coverage
- [ ] Format with rustfmt

### 16.3 CI/CD
- [ ] Set up GitHub Actions
- [ ] Test on multiple Rust versions
- [ ] Test on different platforms (Linux, macOS, Windows)
- [ ] Automated releases with cargo-release

### 16.4 Publication
- [ ] Choose appropriate license (MIT/Apache-2.0)
- [ ] Publish to crates.io
- [ ] Announce on Rust forums/Reddit
- [ ] Add to Awesome Rust list

---

## Estimated Time ‚è±Ô∏è

- **Phase 1-2**: 1-2 days (infrastructure + whitespace/comments)
- **Phase 3-4**: 2-3 days (identifiers + lifetimes)
- **Phase 5**: 3-4 days (numeric literals, tricky!)
- **Phase 6-7**: 4-5 days (char/byte/string literals, many variants)
- **Phase 8-9**: 1-2 days (operators + error cases)
- **Phase 10**: 1 day (main loop integration)
- **Phase 11**: 2-3 days (error handling polish)
- **Phase 12**: 3-5 days (comprehensive testing)
- **Phase 13**: 2-3 days (optimizations)
- **Phase 14**: 2-3 days (API + docs)
- **Phase 15**: Variable (optional features)
- **Phase 16**: 1-2 days (final validation)

**Total**: ~25-35 days for a solid, production-ready lexer

---

## Key Resources üìñ

- [Rust Reference - Lexical Structure](https://doc.rust-lang.org/reference/lexical-structure.html)
- [rustc_lexer source code](https://github.com/rust-lang/rust/tree/master/compiler/rustc_lexer)
- [Unicode XID specification](http://www.unicode.org/reports/tr31/)
- [Rust Edition Guide](https://doc.rust-lang.org/edition-guide/)

---

Good luck building your Rust lexer! ü¶Ä
